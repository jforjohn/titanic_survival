{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename type: train\n",
      "\n",
      "process_type: all\n",
      "process_type: all\n",
      "Index(['Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'PclassCp_2', 'PclassCp_3', 'Title_Mr0', 'Title_Mr50', 'Title_Mrs', 'Title_Ms', 'FamilySize', 'Em_C', 'Em_Q', 'Em_S', 'Cabin_A', 'Cabin_B', 'Cabin_C', 'Cabin_D', 'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_X', 'Age_bin_Kid', 'Age_bin_Teenager', 'Age_bin_Adult', 'Family_bin_SmallFamily', 'Family_bin_BigFamily', 'Family_bin_Team', 'Fare_bin_Median', 'Fare_bin_Average', 'Fare_bin_High', 'Mother', 'Father', 'Daughter', 'Son', 'Orphan', 'RichWoman', 'MiddleClassWoman', 'PoorWoman', 'RichMan', 'MiddleClassMan', 'PoorMan', 'RichGirl', 'MiddleClassGirl', 'PoorGirl', 'RichBoy', 'MiddleClassBoy', 'PoorBoy'], dtype='object') Index(['Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'PclassCp_2', 'PclassCp_3', 'Title_Mr0', 'Title_Mr50', 'Title_Mrs', 'Title_Ms', 'FamilySize', 'Em_C', 'Em_Q', 'Em_S', 'Cabin_A', 'Cabin_B', 'Cabin_C', 'Cabin_D', 'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_X', 'Age_bin_Kid', 'Age_bin_Teenager', 'Age_bin_Adult', 'Family_bin_SmallFamily', 'Family_bin_BigFamily', 'Family_bin_Team', 'Fare_bin_Median', 'Fare_bin_Average', 'Fare_bin_High', 'Mother', 'Father', 'Daughter', 'Son', 'Orphan', 'RichWoman', 'MiddleClassWoman', 'PoorWoman', 'RichMan', 'MiddleClassMan', 'PoorMan', 'RichGirl', 'MiddleClassGirl', 'PoorGirl', 'RichBoy', 'MiddleClassBoy', 'PoorBoy'], dtype='object')\n",
      "(888, 49) (418, 49)\n",
      "Fitting 5 folds for each of 630 candidates, totalling 3150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=4)]: Done 357 tasks      | elapsed:   50.3s\n",
      "[Parallel(n_jobs=4)]: Done 640 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=4)]: Done 1005 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=4)]: Done 1450 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=4)]: Done 1977 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=4)]: Done 2584 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=4)]: Done 3150 out of 3150 | elapsed:  7.7min finished\n",
      "/Users/jesusperalesh/Dropbox/UPC/MasterIA1/IML/Practica/LabEnv/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'activation': 'relu', 'hidden_layer_sizes': 10, 'learning_rate': 'constant', 'learning_rate_init': 0.1, 'solver': 'sgd'}\n",
      "Best score:  0.8344594594594594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jesusperalesh/Dropbox/UPC/MasterIA1/IML/Practica/LabEnv/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "#from MyPreprocessing import MyPreprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from config_loader import load\n",
    "import argparse\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from MyDataUnderstanding import featureAnalysis\n",
    "from MyPreprocessing import MyPreprocessing\n",
    "from MyFeatureSelection import MyFeatureSelection\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import matplotlib.pyplot as plt\n",
    "from model.models import models_perform\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xg\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "def getData(path, filenames_type):\n",
    "    '''\n",
    "    features_lst = [\n",
    "        \"Pclass\", \"Survived\", \"Name\", \"Sex\", \"Age\",\n",
    "        \"Sibsp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\",\"Embarked\"]\n",
    "    '''\n",
    "    if filenames_type == 'train':\n",
    "        filename = 'train'\n",
    "    elif filenames_type == 'test':\n",
    "        filename = 'test'\n",
    "    else:\n",
    "        filename = 'titanicAll'\n",
    "    df_features = pd.read_csv(path + filename + '.csv',\n",
    "                           sep=',')\n",
    "\n",
    "    if filenames_type not in ['train', 'test']:\n",
    "        # drop unnecessary columns that don't exist in the official dataset\n",
    "        df_features.drop(['Boat', 'Body', 'Home.dest'],\n",
    "                          axis=1,\n",
    "                         inplace=True)\n",
    "    #labels = df_features['Survived']\n",
    "    #df_features = df_features.drop(['Survived'], axis=1)\n",
    "    return df_features\n",
    "##\n",
    "if __name__ == '__main__':\n",
    "    ##\n",
    "    # Loads config\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"-c\", \"--config\", default=\"titanic.cfg\",\n",
    "        help=\"specify the location of the clustering config file\"\n",
    "    )\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    config_file = args.config\n",
    "    config = load(config_file)\n",
    "\n",
    "    ##\n",
    "    verbose = config.get('titanic', 'verbose')\n",
    "    path = config.get('titanic', 'path') + '/'\n",
    "    file_type = config.get('titanic', 'file_type')\n",
    "\n",
    "    filename_type = 'train'\n",
    "    if file_type == 'all':\n",
    "        filename_type = 'other'\n",
    "\n",
    "    print('Filename type:', filename_type)\n",
    "    print()\n",
    "    ## train\n",
    "    trainData = getData(path, filename_type)\n",
    "    # Preprocessing\n",
    "    trainPreprocess = MyPreprocessing(process_type='all',\n",
    "                                      filename_type=filename_type,\n",
    "                                      remove_outliers=True)\n",
    "\n",
    "    ## test\n",
    "    filename_type = 'test'\n",
    "    testData = getData(path, filename_type)\n",
    "    # Preprocessing\n",
    "    testPreprocess = MyPreprocessing(process_type='all',\n",
    "                                     filename_type=filename_type,\n",
    "                                     remove_outliers=False)\n",
    "\n",
    "    ## Data Understanding\n",
    "    if verbose == 'true':\n",
    "        featureAnalysis(trainData)\n",
    "        featureAnalysis(testData)\n",
    "\n",
    "\n",
    "    trainPreprocess.fit(trainData)\n",
    "    df_train = trainPreprocess.new_df\n",
    "    # the labels \"Survived\"\n",
    "    labels = trainPreprocess.labels_\n",
    "    #print(labels.head())\n",
    "    # the initial dataset without any preprocessing\n",
    "    #print(trainPreprocess.df_initial.head())\n",
    "    # the preprocessed data\n",
    "    #print(trainPreprocess.new_df.head())\n",
    "\n",
    "    testPreprocess.fit(testData)\n",
    "    df_test = testPreprocess.new_df\n",
    "\n",
    "    # fix missing columns because of NaNs and one hot encoding without dummy_na\n",
    "    if df_train.shape[1] != df_test.shape[1]:\n",
    "        missing_cols = set(df_test.columns) - set(df_train.columns)\n",
    "        for col in missing_cols:\n",
    "            #df_train[col] = np.zeros([df_train.shape[0], 1])\n",
    "            df_test.drop([col], axis=1, inplace=True)\n",
    "\n",
    "        missing_cols = set(df_train.columns) - set(df_test.columns)\n",
    "        for col in missing_cols:\n",
    "            #df_test[col] = np.zeros([df_test.shape[0], 1])\n",
    "            df_train.drop([col], axis=1, inplace=True)\n",
    "\n",
    "    labels_test = testPreprocess.labels_\n",
    "\n",
    "    print(df_train.columns, df_test.columns)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "\n",
    "    '''\n",
    "    Best Feature Selection technique for each model:\n",
    "                                        Accuracy    Time\n",
    "    ALL_49_XGBClassifier\t\t\t\t\t0.822198\t0.148959\n",
    "    ALL_49_SVC\t\t\t\t\t\t\t\t0.821009\t0.036744\n",
    "    ICA_d_25/49RandomForestClassifier\t\t0.817587\t0.023186\n",
    "    ALL_49_LinearDiscriminantAnalysis\t\t0.814099\t0.018538\n",
    "    ALL_49_KNeighborsClassifier\t\t\t\t0.807196\t0.014191\n",
    "    PCA_d_20/49_ev_0.932_MLPClassifier\t\t0.807176\t3.611699\n",
    "    RF_d_25/49MyIBL\t\t\t\t\t\t\t0.778340\t2.562606\n",
    "    '''\n",
    "\n",
    "    #warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    '''ASIERdef xgBoost(data, labels, train_fidx, validation_fidx):\n",
    "        model = xg.XGBClassifier()\n",
    "        folds_accuracy = list()\n",
    "        for idx, trf in enumerate(train_fidx):\n",
    "            model.fit(data.loc[trf], labels.loc[trf])\n",
    "            prediction_labels = model.predict(data.loc[validation_fidx[idx]])\n",
    "\n",
    "            folds_accuracy.append(accuracy_score(labels.loc[validation_fidx[idx]], prediction_labels))\n",
    "        print(\"Support Vector Classifier provided\", mean(folds_accuracy), \"accuracy at validation stage.\")\n",
    "    \n",
    "    # Divide dataset in folds\n",
    "    kf = KFold(n_splits=5)\n",
    "\n",
    "    folds = [(train_idx, validation_idx) for train_idx, validation_idx in kf.split(df_train)]\n",
    "    train_idx = [f[0] for f in folds]\n",
    "    validation_idx = [f[1] for f in folds]\n",
    "    \n",
    "    xgBoost(df_train, labels, train_idx, validation_idx)'''\n",
    "    \n",
    "    # XGBoost\n",
    "    # Best params:  {'colsample_bytree': 0.6, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 350, 'objective': 'binary:logistic', 'subsample': 0.5}\n",
    "    # Best score:  0.8536036036036037\n",
    "    '''configs = [\n",
    "        {'objective':'binary:logistic', 'n_estimators':500, 'max_depth':5, 'learning_rate':0.01, 'gamma':0, 'subsample':0.50, 'colsample_bytree':0.6},\n",
    "        {'objective':'binary:logistic', 'n_estimators':500, 'max_depth':5, 'learning_rate':0.01, 'gamma':0, 'subsample':0.50, 'colsample_bytree':0.6}\n",
    "    ]\n",
    "\n",
    "    param_ranges = {\n",
    "        'objective' : ['binary:logistic', 'binary:hinge'],\n",
    "        'n_estimators' : [150, 200, 250, 300, 350, 400, 450, 500],\n",
    "        'max_depth' : [4, 5, 6, 7],\n",
    "        'learning_rate' : [0.1, 0.01, 0.001, 0.0001],\n",
    "        'subsample' : [0.5, 0.6],\n",
    "        'colsample_bytree' : [0.5, 0.6]\n",
    "    }\n",
    "\n",
    "    xgb = xg.XGBClassifier()\n",
    "\n",
    "    grid_search = GridSearchCV(xgb, param_ranges, cv=5, scoring=\"accuracy\", n_jobs=4, verbose=2)\n",
    "\n",
    "    grid_search.fit(df_train, labels)\n",
    "    \n",
    "    print(\"Best params: \", grid_search.best_params_)\n",
    "    print(\"Best score: \", grid_search.best_score_)'''\n",
    "    \n",
    "    \n",
    "    # Random Forest\n",
    "    # Best params:  {'criterion': 'gini', 'max_depth': 90, 'max_features': 'log2', 'min_samples_leaf': 7, 'min_samples_split': 8, 'n_estimators': 50}\n",
    "    # Best score:  0.8265765765765766\n",
    "    '''param_ranges = {\n",
    "        'criterion' : ['gini', 'entropy'],\n",
    "        'n_estimators' : [50, 100, 150, 200],\n",
    "        'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth' : [80, 90, 100, 110],\n",
    "        'min_samples_leaf' : [3, 4, 5, 6, 7, 8],\n",
    "        'min_samples_split' : [6, 8, 10, 12],\n",
    "    }\n",
    "    \n",
    "    # ICA - 25 dimensions - for feeding Random Forest Classifier\n",
    "    n_dim_ICA = 25\n",
    "    ica_train, ica_test = MyFeatureSelection.applyICA(df_train, df_test, n_dim_ICA)\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    grid_search = GridSearchCV(rf, param_ranges, cv=5, scoring=\"accuracy\", n_jobs=4, verbose=2)\n",
    "\n",
    "    grid_search.fit(ica_train, labels)\n",
    "    \n",
    "    print(\"Best params: \", grid_search.best_params_)\n",
    "    print(\"Best score: \", grid_search.best_score_)'''\n",
    "    \n",
    "    # MLP\n",
    "    # Best params:  {'activation': 'relu', 'hidden_layer_sizes': 10, 'learning_rate': 'constant', 'learning_rate_init': 0.1, 'solver': 'sgd'}\n",
    "    # Best score:  0.8344594594594594\n",
    "    param_ranges = {\n",
    "        'activation' : ['tanh', 'relu'],\n",
    "        'solver' : ['lbfgs', 'sgd', 'adam'],\n",
    "        'learning_rate' : ['constant', 'invscaling', 'adaptive'],\n",
    "        'learning_rate_init' : [0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "        'hidden_layer_sizes' : [9, 10, 11, 12, 13, 14, 15]\n",
    "    }\n",
    "    \n",
    "    # PCA - 20 dimensions - for feeding MLP\n",
    "    n_dim_PCA = 20\n",
    "    pca_train, pca_test, ev = MyFeatureSelection.applyPCA(df_train, df_test, n_dim_PCA)\n",
    "\n",
    "    mlp = MLPClassifier()\n",
    "\n",
    "    grid_search = GridSearchCV(mlp, param_ranges, cv=5, scoring=\"accuracy\", n_jobs=4, verbose=2)\n",
    "\n",
    "    grid_search.fit(pca_train, labels)\n",
    "    \n",
    "    print(\"Best params: \", grid_search.best_params_)\n",
    "    print(\"Best score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
